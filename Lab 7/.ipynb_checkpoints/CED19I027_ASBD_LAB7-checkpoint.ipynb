{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688d9663",
   "metadata": {},
   "source": [
    "## ASBD LAB 7 - CED19I027 - N. SREE DHYUTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9bf391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CED19I027 - N. SREE DHYUTI - ASBD LAB7\n",
    "\n",
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from openpyxl.styles import PatternFill\n",
    "import squarify\n",
    "import stemgraphic\n",
    "import plotly.graph_objects as go  \n",
    "import plotly.express as px\n",
    "import scipy.stats as stats\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6232a7f4",
   "metadata": {},
   "source": [
    "## 1. Test drive the basic version of FP Growth algorithms for Frequent Itemset Mining using the package / library support in the platform of your choice. Test it with various support and confidence measures and generate a time comparison for varied data set sizes. To do the performance comparison you may use benchmark datasets provided for FIM such as the FIMI workshop or other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c89a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shrimp</th>\n",
       "      <th>almonds</th>\n",
       "      <th>avocado</th>\n",
       "      <th>vegetables mix</th>\n",
       "      <th>green grapes</th>\n",
       "      <th>whole weat flour</th>\n",
       "      <th>yams</th>\n",
       "      <th>cottage cheese</th>\n",
       "      <th>energy drink</th>\n",
       "      <th>tomato juice</th>\n",
       "      <th>low fat yogurt</th>\n",
       "      <th>green tea</th>\n",
       "      <th>honey</th>\n",
       "      <th>salad</th>\n",
       "      <th>mineral water</th>\n",
       "      <th>salmon</th>\n",
       "      <th>antioxydant juice</th>\n",
       "      <th>frozen smoothie</th>\n",
       "      <th>spinach</th>\n",
       "      <th>olive oil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>burgers</td>\n",
       "      <td>meatballs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chutney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>turkey</td>\n",
       "      <td>avocado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mineral water</td>\n",
       "      <td>milk</td>\n",
       "      <td>energy bar</td>\n",
       "      <td>whole wheat rice</td>\n",
       "      <td>green tea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>low fat yogurt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7495</th>\n",
       "      <td>butter</td>\n",
       "      <td>light mayo</td>\n",
       "      <td>fresh bread</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7496</th>\n",
       "      <td>burgers</td>\n",
       "      <td>frozen vegetables</td>\n",
       "      <td>eggs</td>\n",
       "      <td>french fries</td>\n",
       "      <td>magazines</td>\n",
       "      <td>green tea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7497</th>\n",
       "      <td>chicken</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>escalope</td>\n",
       "      <td>green tea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7499</th>\n",
       "      <td>eggs</td>\n",
       "      <td>frozen smoothie</td>\n",
       "      <td>yogurt cake</td>\n",
       "      <td>low fat yogurt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7500 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              shrimp            almonds      avocado    vegetables mix  \\\n",
       "0            burgers          meatballs         eggs               NaN   \n",
       "1            chutney                NaN          NaN               NaN   \n",
       "2             turkey            avocado          NaN               NaN   \n",
       "3      mineral water               milk   energy bar  whole wheat rice   \n",
       "4     low fat yogurt                NaN          NaN               NaN   \n",
       "...              ...                ...          ...               ...   \n",
       "7495          butter         light mayo  fresh bread               NaN   \n",
       "7496         burgers  frozen vegetables         eggs      french fries   \n",
       "7497         chicken                NaN          NaN               NaN   \n",
       "7498        escalope          green tea          NaN               NaN   \n",
       "7499            eggs    frozen smoothie  yogurt cake    low fat yogurt   \n",
       "\n",
       "     green grapes whole weat flour yams cottage cheese energy drink  \\\n",
       "0             NaN              NaN  NaN            NaN          NaN   \n",
       "1             NaN              NaN  NaN            NaN          NaN   \n",
       "2             NaN              NaN  NaN            NaN          NaN   \n",
       "3       green tea              NaN  NaN            NaN          NaN   \n",
       "4             NaN              NaN  NaN            NaN          NaN   \n",
       "...           ...              ...  ...            ...          ...   \n",
       "7495          NaN              NaN  NaN            NaN          NaN   \n",
       "7496    magazines        green tea  NaN            NaN          NaN   \n",
       "7497          NaN              NaN  NaN            NaN          NaN   \n",
       "7498          NaN              NaN  NaN            NaN          NaN   \n",
       "7499          NaN              NaN  NaN            NaN          NaN   \n",
       "\n",
       "     tomato juice low fat yogurt green tea honey salad mineral water salmon  \\\n",
       "0             NaN            NaN       NaN   NaN   NaN           NaN    NaN   \n",
       "1             NaN            NaN       NaN   NaN   NaN           NaN    NaN   \n",
       "2             NaN            NaN       NaN   NaN   NaN           NaN    NaN   \n",
       "3             NaN            NaN       NaN   NaN   NaN           NaN    NaN   \n",
       "4             NaN            NaN       NaN   NaN   NaN           NaN    NaN   \n",
       "...           ...            ...       ...   ...   ...           ...    ...   \n",
       "7495          NaN            NaN       NaN   NaN   NaN           NaN    NaN   \n",
       "7496          NaN            NaN       NaN   NaN   NaN           NaN    NaN   \n",
       "7497          NaN            NaN       NaN   NaN   NaN           NaN    NaN   \n",
       "7498          NaN            NaN       NaN   NaN   NaN           NaN    NaN   \n",
       "7499          NaN            NaN       NaN   NaN   NaN           NaN    NaN   \n",
       "\n",
       "     antioxydant juice frozen smoothie spinach  olive oil  \n",
       "0                  NaN             NaN     NaN        NaN  \n",
       "1                  NaN             NaN     NaN        NaN  \n",
       "2                  NaN             NaN     NaN        NaN  \n",
       "3                  NaN             NaN     NaN        NaN  \n",
       "4                  NaN             NaN     NaN        NaN  \n",
       "...                ...             ...     ...        ...  \n",
       "7495               NaN             NaN     NaN        NaN  \n",
       "7496               NaN             NaN     NaN        NaN  \n",
       "7497               NaN             NaN     NaN        NaN  \n",
       "7498               NaN             NaN     NaN        NaN  \n",
       "7499               NaN             NaN     NaN        NaN  \n",
       "\n",
       "[7500 rows x 20 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\dhyut\\SEM8\\ASBD LAB\\Lab 7\\Market_Basket_Optimisation.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traversal through database for once took 4 mins\n",
    "import numpy as np\n",
    "# Gather All Items of Each Transactions into Numpy Array\n",
    "transaction = []\n",
    "for i in range(0, data.shape[0]):\n",
    "    for j in range(0, data.shape[1]):\n",
    "        transaction.append(data.values[i,j])\n",
    "# converting to numpy array\n",
    "transaction = np.array(transaction)\n",
    "print(transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Transform Them a Pandas DataFrame\n",
    "df = pd.DataFrame(transaction, columns=[\"items\"]) \n",
    "# Put 1 to Each Item For Making Countable Table, to be able to perform Group By\n",
    "df[\"incident_count\"] = 1 \n",
    "#  Delete NaN Items from Dataset\n",
    "indexNames = df[df['items'] == \"nan\" ].index\n",
    "df.drop(indexNames , inplace=True)\n",
    "# Making a New Appropriate Pandas DataFrame for Visualizations  \n",
    "df_table = df.groupby(\"items\").sum().sort_values(\"incident_count\", ascending=False).reset_index()\n",
    "#  Initial Visualizations\n",
    "df_table.head(5).style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f50a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required module\n",
    "import plotly.express as px\n",
    "# to have a same origin\n",
    "df_table[\"all\"] = \"Top 50 items\" \n",
    "# creating tree map using plotly\n",
    "fig = px.treemap(df_table.head(50), path=['all', \"items\"], values='incident_count',\n",
    "                  color=df_table[\"incident_count\"].head(50), hover_data=['items'],\n",
    "                  color_continuous_scale='Blues',\n",
    "                )\n",
    "# ploting the treemap\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d4b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Every Transaction to Seperate List & Gather Them into Numpy Array\n",
    "transaction = []\n",
    "for i in range(data.shape[0]):\n",
    "    transaction.append([str(data.values[i,j]) for j in range(data.shape[1])])\n",
    "# creating the numpy array of the transactions\n",
    "transaction = np.array(transaction)\n",
    "# importing the required module\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "# initializing the transactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transaction).transform(transaction)\n",
    "dataset = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "# dataset after encoded\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select top 30 items\n",
    "first30 = df_table[\"items\"].head(30).values \n",
    "# Extract Top 30\n",
    "dataset = dataset.loc[:,first30] \n",
    "# shape of the dataset\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns.fpgrowth import fpgrowth\n",
    "#running the fpgrowth algorithm\n",
    "res=fpgrowth(dataset,min_support=0.05, use_colnames=True)\n",
    "# printing top 10\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d205641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required module\n",
    "from mlxtend.frequent_patterns.association_rules import association_rules\n",
    "# creating asssociation rules\n",
    "res2=association_rules(res, metric=\"lift\", min_threshold=1)\n",
    "# printing association rules\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471b972",
   "metadata": {},
   "source": [
    "## 2. Extend the Apriori Algorithm discussed in the class supporting Transaction Reduction approach to improve the time complexity issue as a result of the repeated scans limitation of Apriori. You may compare this extended version with the earlier implementations in (1) over the same benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5567ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skmine\n",
    "from skmine.datasets.fimi import fetch_iris\n",
    "iris = fetch_iris()\n",
    "iris_list=[]\n",
    "for i in iris:\n",
    "    iris_list.append(i)\n",
    "print(\"Length of dataset is :\",len(iris_list))\n",
    "iris=pd.DataFrame(iris)\n",
    "#iris\n",
    "print(len(iris.values[0,0]))\n",
    "print(iris.values[0,0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc5fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traversal through database for once took 4 mins\n",
    "import numpy as np\n",
    "# Gather All Items of Each Transactions into Numpy Array\n",
    "transaction = []\n",
    "for i in range(0, iris.shape[0]):\n",
    "    for j in range(0, iris.shape[1]):\n",
    "        for k in range(0, len(iris.values[i,j])):\n",
    "            transaction.append(iris.values[i,j][k])\n",
    "# converting to numpy array\n",
    "transaction = np.array(transaction)\n",
    "print(transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e622671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Transform Them a Pandas DataFrame\n",
    "df = pd.DataFrame(transaction, columns=[\"items\"]) \n",
    "# Put 1 to Each Item For Making Countable Table, to be able to perform Group By\n",
    "df[\"incident_count\"] = 1 \n",
    "#  Delete NaN Items from Dataset\n",
    "indexNames = df[df['items'] == \"nan\" ].index\n",
    "df.drop(indexNames , inplace=True)\n",
    "# Making a New Appropriate Pandas DataFrame for Visualizations  \n",
    "df_table = df.groupby(\"items\").sum().sort_values(\"incident_count\", ascending=False).reset_index()\n",
    "#  Initial Visualizations\n",
    "df_table.head(5).style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a7839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required module\n",
    "import plotly.express as px\n",
    "# to have a same origin\n",
    "df_table[\"all\"] = \"Top 20 items\" \n",
    "# creating tree map using plotly\n",
    "fig = px.treemap(df_table.head(50), path=['all', \"items\"], values='incident_count',\n",
    "                  color=df_table[\"incident_count\"].head(20), hover_data=['items'],\n",
    "                  color_continuous_scale='Blues',\n",
    "                )\n",
    "# ploting the treemap\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bdef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Gather All Items of Each Transactions into Numpy Array\n",
    "transaction = []\n",
    "for i in range(iris.shape[0]):\n",
    "    for j in range(0, iris.shape[1]):\n",
    "        transaction.append([str(iris.values[i,j][k]) for k in range(0, len(iris.values[i,j]))])        \n",
    "# creating the numpy array of the transactions\n",
    "transaction = np.array(transaction)\n",
    "# importing the required module\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "# initializing the transactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transaction).transform(transaction)\n",
    "dataset = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "# dataset after encoded\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd8b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select top 30 items\n",
    "first10 = df_table[\"items\"].head(10).values\n",
    "first10dup = []\n",
    "for i in range(len(first10)):\n",
    "    first10dup.append(str(first10[i]))  \n",
    "# Extract Top 30\n",
    "dataset = dataset.loc[:,first10dup] \n",
    "# shape of the dataset\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b82ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [[y for y in x if pd.notna(y)] for x in data.values.tolist()]\n",
    "print(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c91fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate frequent 1-itemsets\n",
    "candidate_1_itemsets = []\n",
    "for transaction in transactions:\n",
    "    for item in transaction:\n",
    "        if [item] not in candidate_1_itemsets:\n",
    "            candidate_1_itemsets.append([item])\n",
    "print(len(candidate_1_itemsets))                        \n",
    "print(candidate_1_itemsets)   \n",
    "\n",
    "def prune(itemsets, min_support, transactions):\n",
    "    frequent_itemsets = []\n",
    "    for itemset in itemsets:\n",
    "        support_count = 0\n",
    "        for transaction in transactions:\n",
    "            if set(itemset).issubset(set(transaction)):\n",
    "                support_count += 1\n",
    "        if support_count >= min_support:\n",
    "            frequent_itemsets.append(itemset)\n",
    "    return frequent_itemsets\n",
    "frequent_itemsets = prune(candidate_1_itemsets, 300 , transactions)\n",
    "print(len(frequent_itemsets))\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2fe860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(itemsets, k):\n",
    "    candidates = []\n",
    "    for itemset1 in itemsets:\n",
    "        for itemset2 in itemsets:\n",
    "            if itemset1 != itemset2 and len(set(itemset1) | set(itemset2)) == k:\n",
    "                candidate = sorted(list(set(itemset1) | set(itemset2)))\n",
    "                if candidate not in candidates:\n",
    "                    candidates.append(candidate)\n",
    "    return candidates\n",
    "\n",
    "def flatten(l):\n",
    "    return set([item for sublist in l for item in sublist])\n",
    "\n",
    "k=2\n",
    "while len(frequent_itemsets)>0:\n",
    "    candidate_itemsets=generate_candidates(frequent_itemsets,k)\n",
    "    reduced_transactions = []\n",
    "    for transaction in transactions:\n",
    "        reduced_transaction = [item for item in transaction if item in set(flatten(frequent_itemsets))]\n",
    "        if len(reduced_transaction) > 0:\n",
    "            reduced_transactions.append(reduced_transaction)\n",
    "    frequent_itemsets = prune(candidate_itemsets, 100 , reduced_transactions)\n",
    "    print(len(frequent_itemsets))\n",
    "    print(frequent_itemsets)  \n",
    "    print(\"\\n\")\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ae6de",
   "metadata": {},
   "source": [
    "## 3. Test drive any one implementation in (1) or (2) adopting a Vertical Transaction Database format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59813c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [[y for y in x if pd.notna(y)] for x in data.values.tolist()]\n",
    "# Convert to vertical format\n",
    "data = []\n",
    "for i, row in enumerate(transactions):\n",
    "    for item in row:\n",
    "        data.append({\"transaction_id\": i, \"item\": item})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "pivot_table = pd.pivot_table(df, values=\"transaction_id\", index=\"item\", columns=\"transaction_id\", aggfunc=lambda x: 1 if len(x)>0 else 0, fill_value=0)\n",
    "print(pivot_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82612b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "row=[]\n",
    "table=[]\n",
    "print(type(pivot_table.values))\n",
    "for i in range(pivot_table.shape[0]):\n",
    "    for j in range(pivot_table.shape[1]):\n",
    "        if(pivot_table.values[i][j]):\n",
    "            row.append(j+1)   \n",
    "    table.append(row)\n",
    "    row=[]\n",
    "print(table)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff874496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate frequent 1-itemsets\n",
    "candidate_1_itemsets = []\n",
    "for transaction in table:\n",
    "    for item in transaction:\n",
    "        if [item] not in candidate_1_itemsets:\n",
    "            candidate_1_itemsets.append([item])\n",
    "print(len(candidate_1_itemsets))                        \n",
    "print(candidate_1_itemsets)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(itemsets, min_support, table):\n",
    "    frequent_itemsets = []\n",
    "    for itemset in itemsets:\n",
    "        support_count = 0\n",
    "        for transaction in table:\n",
    "            if set(itemset).issubset(set(transaction)):\n",
    "                support_count += 1\n",
    "        if support_count >= min_support:\n",
    "            print(support_count)\n",
    "            frequent_itemsets.append(itemset)\n",
    "    return frequent_itemsets\n",
    "frequent_itemsets = prune(candidate_1_itemsets, 15 , table)\n",
    "print(len(frequent_itemsets))\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd307286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(itemsets, k):\n",
    "    candidates = []\n",
    "    for itemset1 in itemsets:\n",
    "        for itemset2 in itemsets:\n",
    "            if itemset1 != itemset2 and len(set(itemset1) | set(itemset2)) == k:\n",
    "                candidate = sorted(list(set(itemset1) | set(itemset2)))\n",
    "                if candidate not in candidates:\n",
    "                    candidates.append(candidate)\n",
    "    return candidates\n",
    "\n",
    "'''candidate_2_itemsets = generate_candidates(frequent_itemsets,2)\n",
    "frequent_itemsets = prune(candidate_2_itemsets, 300 , transactions)\n",
    "print(frequent_itemsets)\n",
    "print(\"\\n\")'''\n",
    "\n",
    "def flatten(l):\n",
    "    return set([item for sublist in l for item in sublist])\n",
    "\n",
    "k=2\n",
    "while len(frequent_itemsets)>0:  \n",
    "    candidate_itemsets=generate_candidates(frequent_itemsets,k)\n",
    "    reduced_transactions = []\n",
    "    for transaction in table:\n",
    "        reduced_transaction = [item for item in transaction if item in set(flatten(frequent_itemsets))]\n",
    "        if len(reduced_transaction) > 0:\n",
    "            reduced_transactions.append(reduced_transaction)\n",
    "    frequent_itemsets = prune(candidate_itemsets, 5 , reduced_transactions)\n",
    "    print(frequent_itemsets)  \n",
    "    print(\"\\n\")\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a8beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de49cf48",
   "metadata": {},
   "source": [
    "## 4. Using a vertical transaction database notation, generate the FI’s following the intersection approach (basic ECLAT) discussed in the class. Use earlier benchmark datasets in (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da12c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.values returns numpy array \n",
    "#tolist converts its to list\n",
    "data = pd.read_csv(r\"C:\\Users\\dhyut\\SEM8\\ASBD LAB\\Lab 7\\Market_Basket_Optimisation.csv\")\n",
    "records = [[y for y in x if pd.notna(y)] for x in data.values.tolist()]\n",
    "print(data.values.tolist())\n",
    "Database={}\n",
    "for i in range(len(records)):\n",
    "    Database[\"T\"+str(i+1)]=records[i]\n",
    "print(Database)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "Database_vdf={}\n",
    "for key,val in Database.items():\n",
    "    for x in val:\n",
    "        if (x,) not in Database_vdf:\n",
    "            Database_vdf[(x,)]=[key]\n",
    "        else:\n",
    "            Database_vdf[(x,)].append(key)\n",
    "print(Database_vdf)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d66f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_vdf=[]\n",
    "for key,val in Database_vdf.items():\n",
    "    records_vdf.append(val)\n",
    "print(records_vdf)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d304f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "import pandas as pd\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records_vdf).transform(records_vdf)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9caf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_items_vdf(Database_vdf,Min_Sup):\n",
    "    rem_keys=[]\n",
    "    for key,val in Database_vdf.items():\n",
    "        if(len(val)<Min_Sup):\n",
    "            rem_keys.append(key)\n",
    "    for key in rem_keys:\n",
    "        Database_vdf.pop(key)\n",
    "    return Database_vdf\n",
    "\n",
    "def get_vdf(Li,iteration):\n",
    "    New_Li={}\n",
    "    for key1,val1 in Li.items():\n",
    "        for key2,val2 in Li.items():\n",
    "            if(key1!=key2):\n",
    "                new_key=key1+key2\n",
    "                if(len(new_key)>iteration):\n",
    "                    continue\n",
    "                else:\n",
    "                    New_Li[new_key]=set(val1).intersection(set(val2))\n",
    "    return New_Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration =0\n",
    "Min_Sup=300\n",
    "\n",
    "Li=Database_vdf\n",
    "iteration+=1\n",
    "print(\"Iteration No: \",iteration)    \n",
    "Li=remove_items_vdf(Li,Min_Sup)\n",
    "for key1,val1 in Li.items():\n",
    "    print(key1)\n",
    "\n",
    "while(1):\n",
    "    iteration+=1\n",
    "    c=get_vdf(Li,iteration)\n",
    "    print(\"Iteration No: \",iteration)\n",
    "    Li=remove_items_vdf(c,Min_Sup)\n",
    "    for key1,val1 in Li.items():\n",
    "        print(key1)\n",
    "    if(len(Li)==0):\n",
    "        break\n",
    "    else:\n",
    "        final_vdf=Li\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b83df6",
   "metadata": {},
   "source": [
    "## 5 - 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [[y for y in x if pd.notna(y)] for x in data.values.tolist()]\n",
    "\n",
    "print(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate frequent 1-itemsets\n",
    "candidate_1_itemsets = []\n",
    "for transaction in transactions:\n",
    "    for item in transaction:\n",
    "        if [item] not in candidate_1_itemsets:\n",
    "            candidate_1_itemsets.append([item])\n",
    "print(len(candidate_1_itemsets))                        \n",
    "print(candidate_1_itemsets)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6301dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(itemsets, min_support, transactions):\n",
    "    frequent_itemsets = []\n",
    "    for itemset in itemsets:\n",
    "        support_count = 0\n",
    "        for transaction in transactions:\n",
    "            if all(x in transaction for x in itemset):\n",
    "                support_count += 1\n",
    "        if support_count >= min_support:\n",
    "            frequent_itemsets.append(itemset)\n",
    "    return frequent_itemsets\n",
    "frequent_itemsets = prune(candidate_1_itemsets, 300 , transactions)\n",
    "print(len(frequent_itemsets))\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(itemsets, k):\n",
    "    candidates = []\n",
    "    for itemset1 in itemsets:\n",
    "        for itemset2 in itemsets:\n",
    "            if itemset1 != itemset2 and len(set(itemset1) | set(itemset2)) == k:\n",
    "                candidate = list(set(itemset1) | set(itemset2))\n",
    "                if candidate not in candidates:\n",
    "                    candidates.append(candidate)\n",
    "    return candidates\n",
    "\n",
    "'''candidate_2_itemsets = generate_candidates(frequent_itemsets,2)\n",
    "frequent_itemsets = prune(candidate_2_itemsets, 300 , transactions)\n",
    "print(frequent_itemsets)\n",
    "print(\"\\n\")'''\n",
    "\n",
    "def flatten(l):\n",
    "    return set([item for sublist in l for item in sublist])\n",
    "\n",
    "k=2\n",
    "while len(frequent_itemsets)>0:\n",
    "    candidate_itemsets=generate_candidates(frequent_itemsets,k)\n",
    "    reduced_transactions = []\n",
    "    for transaction in transactions:\n",
    "        reduced_transaction = [item for item in transaction if item in set(flatten(frequent_itemsets))]\n",
    "        if len(reduced_transaction) > 0:\n",
    "            reduced_transactions.append(reduced_transaction)\n",
    "    frequent_itemsets = prune(candidate_itemsets, 100 , reduced_transactions)\n",
    "    print(len(frequent_itemsets))\n",
    "    print(frequent_itemsets)  \n",
    "    print(\"\\n\")\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d96ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = [[1,1,0],[1,0,0],[0,1,1],[0,0,0]]\n",
    "unique_itemset =[{1},{2},{3}]\n",
    "min_supp = 1\n",
    "M = 2\n",
    "size = len(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools \n",
    "import copy\n",
    "\n",
    "def get_subset(S,n):\n",
    "    a = itertools.combinations(S,n)\n",
    "    results = []\n",
    "    for i in a:\n",
    "        results.append(set(i))\n",
    "    return(results)\n",
    "\n",
    "def get_superset(S,unique_itemset):\n",
    "    result = []\n",
    "    a = set()\n",
    "    for i in unique_itemset:\n",
    "        if i.intersection(S)==set():\n",
    "            a = i.union(S)\n",
    "            result.append(a)\n",
    "            a = set()\n",
    "\n",
    "    return(result)\n",
    "\n",
    "def check_subset(Set,frequent_set):\n",
    "    subset = get_subset(Set,len(Set)-1)\n",
    "    flag = 1\n",
    "    temp = []\n",
    "\n",
    "    for i in frequent_set:\n",
    "        temp.append(i[0])\n",
    "\n",
    "    frequent_set = temp\n",
    "    for i in subset:\n",
    "        if i not in frequent_set:\n",
    "            flag=0\n",
    "            break\n",
    "\n",
    "    if flag:\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "def get_itemset(T):\n",
    "    result = set()\n",
    "    for i in range(len(T)):\n",
    "        if T[i]!=0:\n",
    "            result.add(i+1)\n",
    "\n",
    "    return(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e589062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_itemset_counting(database, min_supp, unique_itemset, M):\n",
    "    DC = []\n",
    "    DS = []\n",
    "    SC = []\n",
    "    SS = []\n",
    "\n",
    "    for i in unique_itemset:\n",
    "        DC.append([i,0,0])\n",
    "\n",
    "    print(\"Initial DC:\",DC,\"\\n\")\n",
    "    counter = 0\n",
    "    T = []\n",
    "    frequent_itemsets = []\n",
    "    size = len(database)\n",
    "\n",
    "    while len(DC)!=0 or len(DS)!=0:\n",
    "\n",
    "        for i in range(counter,counter+M):\n",
    "            index = i%size\n",
    "            T = get_itemset(database[index])\n",
    "            print(\"Transaction :\",T)\n",
    "\n",
    "            for item in DC:\n",
    "                item[2]+=1\n",
    "                if item[0].issubset(T):\n",
    "                    item[1]+=1\n",
    "            for item in DS:\n",
    "                item[2]+=1\n",
    "                if item[0].issubset(T):\n",
    "                    item[1]+=1\n",
    "\n",
    "        for item in copy.copy(DC):\n",
    "            if(item[1]>=min_supp):\n",
    "                DS.append(item)\n",
    "                DC.remove(item)\n",
    "\n",
    "        for item in copy.copy(DS):\n",
    "            if(item[2]==size):\n",
    "                SS.append(item)\n",
    "                DS.remove(item)\n",
    "        for item in copy.copy(DC):\n",
    "            if(item[2]==size):\n",
    "                SC.append(item)\n",
    "                DC.remove(item)\n",
    "\n",
    "        frequent_set = copy.copy(DS)\n",
    "        frequent_set.extend(SS)\n",
    "        for item in frequent_set:\n",
    "            S = get_superset(item[0],unique_itemset)\n",
    "            for i in S:\n",
    "                if (check_subset(i,frequent_set)):\n",
    "                    flag=1\n",
    "                    for x in DC:\n",
    "                        if x[0]==i:\n",
    "                            flag=0\n",
    "                    for x in DS:\n",
    "                        if x[0]==i:\n",
    "                            flag=0\n",
    "                    for x in SC:\n",
    "                        if x[0]==i:\n",
    "                            flag=0\n",
    "                    for x in SS:\n",
    "                        if x[0]==i:\n",
    "                            flag=0\n",
    "                    if flag:\n",
    "                        DC.append([i,0,0])\n",
    "\n",
    "        counter+=M\n",
    "        print(\"DS: \",DS)\n",
    "        print(\"DC: \",DC)\n",
    "        print(\"SS: \",SS)\n",
    "        print(\"SC: \",SC,\"\\n\")\n",
    "\n",
    "    for item in DS:\n",
    "        if item[1] >= min_supp:\n",
    "            frequent_itemsets.append(item[0])\n",
    "\n",
    "    for item in SS:\n",
    "        if item[1] >= min_supp:\n",
    "            frequent_itemsets.append(item[0])\n",
    "\n",
    "    return frequent_itemsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f88b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_itemset_counting(database, min_supp, unique_itemset, M)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
