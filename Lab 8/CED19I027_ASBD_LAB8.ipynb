{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c404c8d",
   "metadata": {},
   "source": [
    "## ASBD LAB 8 - CED19I027 - N. SREE DHYUTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccca34ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CED19I027 - N. SREE DHYUTI - ASBD LAB8\n",
    "\n",
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from openpyxl.styles import PatternFill\n",
    "import squarify\n",
    "import stemgraphic\n",
    "import plotly.graph_objects as go  \n",
    "import plotly.express as px\n",
    "import scipy.stats as stats\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ab2a4",
   "metadata": {},
   "source": [
    "## 1. Test Drive ACLOSE algorithm to mine closed frequent patterns on a sample dataset of your choice. Test the same on a FIMI benchmark dataset which you have used for Apriori/FP-growth implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cb0c315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{frozenset({'b'}): 7, frozenset({'c', 'b'}): 5, frozenset({'b', 'a'}): 4, frozenset({'b', 'e'}): 3, frozenset({'f', 'b'}): 2, frozenset({'c', 'b', 'a'}): 2, frozenset({'c', 'b', 'e'}): 2, frozenset({'b', 'e', 'a'}): 2}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Define a function to compute the support of each itemset\n",
    "def compute_support(itemsets, transactions):\n",
    "    support = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        for itemset in itemsets:\n",
    "            if set(itemset).issubset(transaction):\n",
    "                support[frozenset(itemset)] += 1\n",
    "    return support\n",
    "\n",
    "# Define the sample dataset\n",
    "transactions = [['a', 'b', 'c','e'], ['a', 'b','c'], ['a', 'b', 'e'], ['a', 'b', 'f'], ['b', 'c','d'], ['b', 'c','e'], ['b', 'c','f']]\n",
    "\n",
    "min_support = 2\n",
    "\n",
    "# Find all frequent itemsets and their support values\n",
    "frequent_itemsets = {}\n",
    "itemsets = set(item for transaction in transactions for item in transaction)\n",
    "for k in range(1, len(itemsets)+1):\n",
    "    candidate_itemsets = [set(itemset) for itemset in itertools.combinations(itemsets, k)]\n",
    "    support = compute_support(candidate_itemsets, transactions)\n",
    "    for itemset in support:\n",
    "        if support[itemset] >= min_support:\n",
    "            frequent_itemsets[frozenset(itemset)] = support[itemset]\n",
    "\n",
    "# Initialize the closed frequent itemsets\n",
    "closed_itemsets = frequent_itemsets.copy()\n",
    "\n",
    "# Compute the closure of each frequent itemset\n",
    "for itemset1 in frequent_itemsets:\n",
    "    for itemset2 in frequent_itemsets:\n",
    "        if itemset1 != itemset2 and itemset1.issubset(itemset2) and frequent_itemsets[itemset1] == frequent_itemsets[itemset2]:\n",
    "            closed_itemsets.pop(itemset1, None)\n",
    "            break\n",
    "\n",
    "# Print the resulting closed frequent itemsets\n",
    "print(closed_itemsets)\n",
    "\n",
    "min_support = 1000\n",
    "\n",
    "# Find all frequent itemsets and their support values\n",
    "frequent_itemsets = {}\n",
    "itemsets = set(item for transaction in transactions for item in transaction)\n",
    "for k in range(1, len(itemsets)+1):\n",
    "    candidate_itemsets = [set(itemset) for itemset in itertools.combinations(itemsets, k)]\n",
    "    support = compute_support(candidate_itemsets, transactions)\n",
    "    for itemset in support:\n",
    "        if support[itemset] >= min_support:\n",
    "            frequent_itemsets[frozenset(itemset)] = support[itemset]\n",
    "\n",
    "# Initialize the closed frequent itemsets\n",
    "closed_itemsets = frequent_itemsets.copy()\n",
    "\n",
    "# Compute the closure of each frequent itemset\n",
    "for itemset1 in frequent_itemsets:\n",
    "    for itemset2 in frequent_itemsets:\n",
    "        if itemset1 != itemset2 and itemset1.issubset(itemset2) and frequent_itemsets[itemset1] == frequent_itemsets[itemset2]:\n",
    "            closed_itemsets.pop(itemset1, None)\n",
    "            break\n",
    "\n",
    "# Print the resulting closed frequent itemsets\n",
    "print(closed_itemsets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad37963",
   "metadata": {},
   "source": [
    "## 2. Test Drive Pincer search to mine maximal frequent patterns on a sample dataset of your choice. Test the same on a FIMI benchmark dataset which you have used for Apriori/FP-growth implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "500ac895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmine.datasets.fimi import fetch_mushroom\n",
    "db = fetch_mushroom()\n",
    "\n",
    "def pincer_search(transactions, min_support):\n",
    "    item_counts = {}\n",
    "    frequent_items = {}\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            if item not in item_counts:\n",
    "                item_counts[item] = 0\n",
    "            item_counts[item] += 1\n",
    "    for item in item_counts:\n",
    "        if item_counts[item] >= min_support:\n",
    "            frequent_items[item] = set([item])\n",
    "    maximal_frequent_itemsets = set()\n",
    "    for itemset in frequent_items.values():\n",
    "        is_maximal = True\n",
    "        for other_itemset in frequent_items.values():\n",
    "            if itemset != other_itemset and itemset.issubset(other_itemset):\n",
    "                is_maximal = False\n",
    "                break\n",
    "        if is_maximal:\n",
    "            maximal_frequent_itemsets.add(frozenset(itemset))\n",
    "    return maximal_frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3b6f0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 17.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "maximal_itemsets = pincer_search(db, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3d22e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 28.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "maximal_itemsets = pincer_search(db, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afdf481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
