{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a17bc8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(freqSet-conseq,'-->',conseq,'conf:',conf)? (apriori.py, line 79)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32mc:\\users\\dhyut\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3398\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  Input \u001b[1;32mIn [3]\u001b[1;36m in \u001b[1;35m<cell line: 5>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from apriori import apriori\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\users\\dhyut\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\apriori.py:79\u001b[1;36m\u001b[0m\n\u001b[1;33m    print freqSet-conseq,'-->',conseq,'conf:',conf\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(freqSet-conseq,'-->',conseq,'conf:',conf)?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from apyori import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64a10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data = pd.read_csv(\"/home/naveen/Desktop/Big Data/Lab 6/store_data.csv\",header=None)\n",
    "store_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b75b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for i in range(0, 7501):\n",
    "    records.append([str(store_data.values[i,j]) for j in range(0, 20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16711d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "\n",
    "fpgrowth(df, min_support=0.1,use_colnames=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd39c5",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "#### Extend the Apriori Algorithm discussed in the class supporting Transaction Reduction approach to improve the time complexity issue as a result of the repeated scans limitation of Apriori. You may compare this extended version with the earlier implementations in (1) over the same benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Market Data\n",
    "data=pd.read_csv(\"/home/naveen/Desktop/Big Data/Lab 6/store_data.csv\")\n",
    "df=pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [[y for y in x if pd.notna(y)] for x in df.values.tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample of a record:\\n\",records[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Database={}\n",
    "for i in range(len(records)):\n",
    "    Database[\"T\"+str(i+1)]=records[i]\n",
    "\n",
    "Itemset={}\n",
    "for i in range(len(records)):\n",
    "    for j in range(len(records[i])):\n",
    "        if(frozenset([records[i][j]]) not in Itemset):\n",
    "            Itemset[frozenset([records[i][j]])]=1\n",
    "        else:\n",
    "            Itemset[frozenset([records[i][j]])]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items(Itemset,no_of_items,cur):\n",
    "    for key,val in Itemset.items():\n",
    "        print(key,\" \",val)\n",
    "\n",
    "Min_Sup_Count=0.005*len(records)\n",
    "\n",
    "Li={}\n",
    "for key,val in Itemset.items():\n",
    "    if(val>=Min_Sup_Count):\n",
    "        Li[key]=val\n",
    "Itemset=Li\n",
    "\n",
    "def check(miniset,Database):\n",
    "    count=0\n",
    "    #print(miniset)\n",
    "    for key,val in Database.items():\n",
    "        \n",
    "        if(frozenset(val).intersection(miniset)==miniset):\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def get_c(Li,Database,itert):\n",
    "    c={}\n",
    "    for key,vals in Li.items():\n",
    "        for key1,vals1 in Li.items():\n",
    "            if (key1!=key):\n",
    "                miniset=key1.union(key)\n",
    "                if(len(miniset)>sot+1):\n",
    "                    continue\n",
    "                count=check(miniset,Database)\n",
    "                c[miniset]=count\n",
    "    return c\n",
    "\n",
    "def remove_transaction(Database,sot):\n",
    "    #print(Database)\n",
    "    rem_keys=[]\n",
    "    for key,val in Database.items():\n",
    "        #print(key,val)\n",
    "        if(len(val)<=sot):\n",
    "            rem_keys.append(key)\n",
    "    for key in rem_keys:\n",
    "        Database.pop(key)\n",
    "    return Database\n",
    "\n",
    "def remove_items(c,Min_Sup_Count):\n",
    "    #print(Database)\n",
    "    #print(\"Li=\",c)\n",
    "    rem_keys=[]\n",
    "    for key,val in c.items():\n",
    "        if(val<Min_Sup_Count):\n",
    "            #print(key,val)\n",
    "            rem_keys.append(key)\n",
    "    for key in rem_keys:\n",
    "        c.pop(key)\n",
    "    #print(c)\n",
    "    return c\n",
    "\n",
    "\n",
    "def remove_item(Li,Database):\n",
    "    miniset=set()\n",
    "    for key,val in Li.items():\n",
    "        miniset=miniset.union(key)\n",
    "    for key,val in Database.items():\n",
    "        Database[key]=list(set(val) & miniset)\n",
    "        \n",
    "    return Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b6272",
   "metadata": {},
   "source": [
    "### Print Sample Itemset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6194d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "countitem=0\n",
    "for key,val in Itemset.items():\n",
    "    print(key,\" \",val)\n",
    "    countitem+=1\n",
    "    if(countitem>10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c64a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.process_time()\n",
    "\n",
    "\n",
    "Final_List=[]\n",
    "sot=1\n",
    "final_c={}\n",
    "while(1):\n",
    "    print(\"Iteration No: \",sot)\n",
    "    Database=remove_transaction(Database,sot)\n",
    "    c=get_c(Li,Database,sot+1)\n",
    "    #print(c)\n",
    "    sot+=1\n",
    "    \n",
    "    Li=remove_items(c,Min_Sup_Count)\n",
    "    Database=remove_item(Li,Database)\n",
    "    if(len(Li)==0):\n",
    "        break\n",
    "    else:\n",
    "        final_c=Li\n",
    "\n",
    "time_taken=time.process_time() - start\n",
    "print(\"\\n Time Taken for Mining the itemset with min_support of \" + str(Min_Sup_Count) +\" = \"+str(time_taken)+\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44f47b",
   "metadata": {},
   "source": [
    "### Print Partial Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8452ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "countitem=0\n",
    "for key,val in final_c.items():\n",
    "    print(key,\" \",val)\n",
    "    countitem+=1\n",
    "    if(countitem>10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db41aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "import pandas as pd\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b68b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.process_time()\n",
    "\n",
    "print(apriori(df, min_support=0.005,use_colnames=True))\n",
    "\n",
    "time_taken=time.process_time() - start\n",
    "print(\"\\n Time Taken for Mining using Apriori =\",time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23b870b",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "#### Test drive any one implementation in (1) or (2) adopting a Vertical Transaction Database format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "records=[[100,400,500,700,800,900],[100,200,300,400,600,800,900],[300,500,600,700,800,900],[200,400],[100,800]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Database={}\n",
    "for i in range(len(records)):\n",
    "    Database[\"T\"+str(i+1)]=records[i]\n",
    "\n",
    "Itemset={}\n",
    "for i in range(len(records)):\n",
    "    for j in range(len(records[i])):\n",
    "        if(frozenset([records[i][j]]) not in Itemset):\n",
    "            Itemset[frozenset([records[i][j]])]=1\n",
    "        else:\n",
    "            Itemset[frozenset([records[i][j]])]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6687509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "import pandas as pd\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Database_vdf={}\n",
    "for key,val in Database.items():\n",
    "    for x in val:\n",
    "        if(frozenset([x]) not in Database_vdf):\n",
    "            Database_vdf[frozenset([x])]=frozenset([key])\n",
    "        else:\n",
    "            Database_vdf[frozenset([x])]=frozenset([key]).union(Database_vdf[frozenset([x])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df5b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_vdf=[]\n",
    "for key,val in Database_vdf.items():\n",
    "    records_vdf.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31433768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "import pandas as pd\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records_vdf).transform(records_vdf)\n",
    "df_vdf = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df_vdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f70fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.process_time()\n",
    "\n",
    "print(apriori(df, min_support=0.003,use_colnames=True))\n",
    "\n",
    "time_taken=time.process_time() - start\n",
    "print(\"\\n Time Taken for Mining using Apriori =\",time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662bb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.process_time()\n",
    "\n",
    "print(apriori(df_vdf, min_support=0.003,use_colnames=True))\n",
    "\n",
    "time_taken=time.process_time() - start\n",
    "print(\"\\n Time Taken for Mining using Apriori (Vertical Transaction Database format)  = \"+str(time_taken)+\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb426de7",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "#### Using a vertical transaction database notation, generate the FIâ€™s following the intersection approach (basic ECLAT) discussed in the class. Use earlier benchmark datasets in (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19aacec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/home/naveen/Desktop/Big Data/Lab 6/store_data.csv\")\n",
    "df=pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d91ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [[y for y in x if pd.notna(y)] for x in df.values.tolist()]\n",
    "\n",
    "Database={}\n",
    "for i in range(len(records)):\n",
    "    Database[\"T\"+str(i+1)]=records[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7763d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Database_vdf={}\n",
    "for key,val in Database.items():\n",
    "    for x in val:\n",
    "        if(frozenset([x]) not in Database_vdf):\n",
    "            Database_vdf[frozenset([x])]=frozenset([key])\n",
    "        else:\n",
    "            Database_vdf[frozenset([x])]=frozenset([key]).union(Database_vdf[frozenset([x])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_vdf=[]\n",
    "for key,val in Database_vdf.items():\n",
    "    records_vdf.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "import pandas as pd\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records_vdf).transform(records_vdf)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7be951",
   "metadata": {},
   "source": [
    "#### Scan the Dataset to convert to vertical form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a487d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Database_vdf={}\n",
    "for key,val in Database.items():\n",
    "    for x in val:\n",
    "        #print(Database_vdf)\n",
    "        if(frozenset([x]) not in Database_vdf):\n",
    "            #print(frozenset([x]))\n",
    "            Database_vdf[frozenset([x])]=frozenset([key])\n",
    "        else:\n",
    "            #print(x)\n",
    "            Database_vdf[frozenset([x])]=frozenset([key]).union(Database_vdf[frozenset([x])])\n",
    "            \n",
    "def remove_items_vdf(Database_vdf,Min_Sup):\n",
    "    rem_keys=[]\n",
    "    for key,val in Database_vdf.items():\n",
    "        if(len(val)<Min_Sup):\n",
    "            rem_keys.append(key)\n",
    "    for key in rem_keys:\n",
    "        Database_vdf.pop(key)\n",
    "    return Database_vdf\n",
    "\n",
    "def get_vdf(Li,iteration):\n",
    "    New_Li={}\n",
    "    for key1,val1 in Li.items():\n",
    "        for key2,val2 in Li.items():\n",
    "            if(key1!=key2):\n",
    "                new_key=key1.union(key2)\n",
    "                if(len(new_key)>iteration):\n",
    "                    continue\n",
    "                else:\n",
    "                    New_Li[new_key]=val1.intersection(val2)\n",
    "    return New_Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbb15dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.process_time()\n",
    "Min_Sup_Count_vdf=0.005*len(records)\n",
    "\n",
    "Li=remove_items_vdf(Database_vdf,Min_Sup_Count_vdf)\n",
    "iteration=1\n",
    "while(1):\n",
    "    iteration+=1\n",
    "    c=get_vdf(Li,iteration)\n",
    "    print(\"Iteration No: \",iteration)\n",
    "    Li=remove_items_vdf(c,Min_Sup_Count_vdf)\n",
    "    if(len(Li)==0):\n",
    "        break\n",
    "    else:\n",
    "        final_vdf=Li\n",
    "\n",
    "time_taken=time.process_time() - start\n",
    "print(\"\\n Time Taken for Mining the itemset with min_support of \"+str(Min_Sup_Count_vdf)+\" = \"+str(time_taken)+\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71efca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "countitem=0\n",
    "for key,val in final_vdf.items():\n",
    "    print(key,\" \",val,\"\\n\")\n",
    "    countitem+=1\n",
    "    if(countitem>10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956a9bb",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "#### Extend the basic Apriori algorithm to generate Frequent Patterns which differentiate ab from ba (ordered patterns generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc4f4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences={}\n",
    "sequences[10]=\"<a(abc)(ac)d(cf)>\"\n",
    "sequences[20]=\"<(ad)c(abc)(ae)>\"\n",
    "sequences[30]=\"<(ef)(ab)(df)cb>\"\n",
    "sequences[40]=\"<eg(af)cbc>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee4263d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 'aabcacdcf', 20: 'adcabcae', 30: 'efabdfcb', 40: 'egafcbc'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key,val in sequences.items():\n",
    "    sequences[key]=val.replace('<','').replace('(','').replace(')','').replace('>','')\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd20b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 8, 'b': 5, 'c': 8, 'd': 3, 'f': 4, 'e': 3, 'g': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c0={}\n",
    "for key in sequences.keys():\n",
    "    for x in sequences[key]:\n",
    "        if(x!='(' and x!=')' and x!='<' and x!='>'):\n",
    "            if(x not in c0):\n",
    "                c0[x]=1\n",
    "            else:\n",
    "                c0[x]+=1\n",
    "c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eecfa6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 8, 'b': 5, 'c': 8, 'd': 3, 'f': 4, 'e': 3, 'g': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l0={}\n",
    "Min_Sup_Count=1\n",
    "for key in c0.keys():\n",
    "    if(c0[key]>=Min_Sup_Count):\n",
    "        l0[key]=c0[key]\n",
    "l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c0c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_sequence(l0,sequences,count):\n",
    "    c1={}\n",
    "    for key1 in l0.keys():\n",
    "        for key2 in l0.keys():\n",
    "            if(key1!=key2):\n",
    "                new_key=key1+key2\n",
    "                if(len(new_key)==count):\n",
    "                    for key in sequences.keys():\n",
    "                        if(new_key not in c1):\n",
    "                            c1[new_key]=len(re.findall(new_key,sequences[key]))\n",
    "                        else:\n",
    "                            c1[new_key]+=len(re.findall(new_key,sequences[key]))\n",
    "    return c1\n",
    "                    \n",
    "                    \n",
    "def remove_sequence(c,Min_Sup_Count):\n",
    "    Li={}\n",
    "    for key in c.keys():\n",
    "        if(c[key]>=Min_Sup_Count):\n",
    "            Li[key]=c[key]\n",
    "    return Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de945f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No:  1\n",
      "Iteration No:  2\n"
     ]
    }
   ],
   "source": [
    "sot=1\n",
    "final_sequence={}\n",
    "Min_Sup_Count=3\n",
    "while(1):\n",
    "    print(\"Iteration No: \",sot)\n",
    "\n",
    "    c=get_sequence(l0,sequences,sot+1)\n",
    "    \n",
    "    sot+=1\n",
    "    Li=remove_sequence(c,Min_Sup_Count)\n",
    "    if(len(Li)==0):\n",
    "        break\n",
    "    else:\n",
    "        final_sequence=Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a3e31f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Patterns are: ['ab', 'bc', 'ca']\n"
     ]
    }
   ],
   "source": [
    "print(\"Frequent Patterns are:\",[x for x in final_sequence.keys()] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29bbe2",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "#### Implement following extensions to Apriori Algorithm (discussed / to be discussed in the class): Hash based strategy, Partitioning Approach. You may refer to online tutorials for a formal pseudocode description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c22c6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transactions = [{1, 2, 5},{2, 4},{2, 3},{1, 2, 4},{1, 3},{2, 3},{1, 3},{1, 2, 3, 5},{1, 2, 3},{1, 2},{1, 3, 5}]\n",
    "transactions = [{'A','C','D'},{'B','C','E'},{'A','B','C','E'},{'B','E'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7912ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_hash={}\n",
    "count=0\n",
    "for i in transactions:\n",
    "    count+=1\n",
    "    database_hash[\"T\"+str(count)]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1d76b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D': 1, 'C': 3, 'A': 2, 'B': 3, 'E': 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c0={}\n",
    "for i in transactions:\n",
    "    for j in i:\n",
    "        if(j in c0):\n",
    "            c0[j]+=1\n",
    "        else:\n",
    "            c0[j]=1\n",
    "c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7efba8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order={}\n",
    "count=0\n",
    "for key in sorted(c0.keys()):\n",
    "    count+=1\n",
    "    order[key]=count\n",
    "order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba00317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 3, 'A': 2, 'B': 3, 'E': 3}\n"
     ]
    }
   ],
   "source": [
    "Min_Sup_Count=2\n",
    "rem_keys=[]\n",
    "Li={}\n",
    "for key,val in c0.items():\n",
    "    if(val>=Min_Sup_Count):\n",
    "        Li[key]=val\n",
    "        rem_keys.append(key)\n",
    "\n",
    "print(Li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b8948ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T1': [['C', 'D'], ['A', 'D'], ['A', 'C']],\n",
       " 'T2': [['B', 'E'], ['B', 'C'], ['C', 'E']],\n",
       " 'T3': [['B', 'E'],\n",
       "  ['B', 'C'],\n",
       "  ['A', 'B'],\n",
       "  ['C', 'E'],\n",
       "  ['A', 'E'],\n",
       "  ['A', 'C']],\n",
       " 'T4': [['B', 'E']]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "for key,val in database_hash.items():\n",
    "    val=[set(i) for i in itertools.combinations(val, 2)]\n",
    "    sets=[]\n",
    "    for i in range(len(val)):\n",
    "        sets.append(sorted(val[i]))\n",
    "\n",
    "    database_hash[key]=sets\n",
    "database_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff8d727a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6: [['C', 'D'], ['A', 'C'], ['A', 'C']],\n",
       " 0: [['A', 'D'], ['C', 'E'], ['C', 'E']],\n",
       " 4: [['B', 'E'], ['B', 'E'], ['B', 'E']],\n",
       " 2: [['B', 'C'], ['B', 'C']],\n",
       " 5: [['A', 'B']],\n",
       " 1: [['A', 'E']]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate hash table\n",
    "Hash_Table={}\n",
    "for key,items in database_hash.items():\n",
    "    for x in items:\n",
    "        val=(order[x[0]]*10+order[x[1]])%7\n",
    "        if(val in Hash_Table):\n",
    "            Hash_Table[val].append(x)\n",
    "        else:\n",
    "            Hash_Table[val]=[x]\n",
    "Hash_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29d7120c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 3, 'A': 2, 'B': 3, 'E': 3}\n"
     ]
    }
   ],
   "source": [
    "print(Li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c31d3d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{frozenset({'B', 'A'}): 1, frozenset({'C', 'A'}): 3, frozenset({'E', 'A'}): 1, frozenset({'B', 'C'}): 2, frozenset({'B', 'E'}): 3, frozenset({'E', 'C'}): 3}\n"
     ]
    }
   ],
   "source": [
    "#Generate C2\n",
    "C2={}\n",
    "keys=sorted(Li.keys())\n",
    "for i in range(len(keys)):\n",
    "    for j in range(i+1,len(keys)):\n",
    "        New_key=frozenset(set(keys[i]).union(set(keys[j])))\n",
    "        New_val=(order[keys[i]]*10+order[keys[j]])%7\n",
    "        C2[New_key]=len(Hash_Table[New_val])\n",
    "print(C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19bcbc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{frozenset({'C', 'A'}): 3, frozenset({'B', 'C'}): 2, frozenset({'B', 'E'}): 3, frozenset({'E', 'C'}): 3}\n"
     ]
    }
   ],
   "source": [
    "#Generate L2\n",
    "L2={}\n",
    "for key,val in C2.items():\n",
    "    if(val>=Min_Sup_Count):\n",
    "        L2[key]=val\n",
    "\n",
    "print(L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb2c5a",
   "metadata": {},
   "source": [
    "#### Partitioning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "764959e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/naveen/Desktop/Big Data/Lab 6/store_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/naveen/Desktop/Big Data/Lab 6/store_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\n",
      "File \u001b[1;32mc:\\users\\dhyut\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py:605\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    600\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    601\u001b[0m     dialect, delimiter, delim_whitespace, engine, sep, defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    602\u001b[0m )\n\u001b[0;32m    603\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\dhyut\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py:457\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    454\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    456\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 457\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\users\\dhyut\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py:814\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\dhyut\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py:1045\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1042\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1043\u001b[0m     )\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[1;32m-> 1045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\dhyut\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py:1862\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1859\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\users\\dhyut\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py:1357\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1354\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1355\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHanldes after they are done with their potential raises.\u001b[39;00m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\dhyut\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\common.py:639\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 639\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    647\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    648\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/naveen/Desktop/Big Data/Lab 6/store_data.csv'"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"/home/naveen/Desktop/Big Data/Lab 6/store_data.csv\",header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a845944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [[y for y in x if pd.notna(y)] for x in df.values.tolist()]\n",
    "\n",
    "Database={}\n",
    "for i in range(len(records)):\n",
    "    Database[\"T\"+str(i+1)]=records[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b103fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "import pandas as pd\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40484be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dfs(df,no):\n",
    "    dfs=[]\n",
    "    for i in range(0,df.shape[0],int(df.shape[0]/no)):\n",
    "        dfs.append(df.iloc[i:i+int(df.shape[0]/no)])\n",
    "    return dfs\n",
    "dfs=split_dfs(df,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1052855",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for i in dfs:\n",
    "    results.append(apriori(i, min_support=0.01,use_colnames=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15770bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "final_candidate_set={}\n",
    "for i in results:\n",
    "    for j in range(i.shape[0]):\n",
    "        item=i.iloc[j][1]\n",
    "        if(item in final_candidate_set):\n",
    "            final_candidate_set[item]+=(i.iloc[j][0]*int(df.shape[0]/13))\n",
    "        else:\n",
    "            final_candidate_set[item]=(i.iloc[j][0]*int(df.shape[0]/13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da9c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results={}\n",
    "Min_Sup_Count=int(df.shape[0]*(0.1))\n",
    "for key,val in final_candidate_set.items():\n",
    "    if(val>=Min_Sup_Count):\n",
    "        final_results[key]=val\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67a114a",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "#### Implement the Dynamic Itemset Counting Algorithm for Frequent Itemset Generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc62d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = [[1,1,0],[1,0,0],[0,1,1],[0,0,0]]\n",
    "unique_itemset =[{1},{2},{3}]\n",
    "min_supp = 1\n",
    "M = 2\n",
    "size = len(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4916fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools \n",
    "import copy\n",
    "\n",
    "def get_subset(S,n):\n",
    "    a = itertools.combinations(S,n)\n",
    "    results = []\n",
    "    for i in a:\n",
    "        results.append(set(i))\n",
    "    return(results)\n",
    "\n",
    "def get_superset(S,unique_itemset):\n",
    "    #print(S)\n",
    "    result = []\n",
    "    a = set()\n",
    "    for i in unique_itemset:\n",
    "        if i.intersection(S)==set():\n",
    "            a = i.union(S)\n",
    "            result.append(a)\n",
    "            a = set()\n",
    "\n",
    "    return(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f1f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_subset(Set,frequent_set):\n",
    "    subset = get_subset(Set,len(Set)-1)\n",
    "    flag = 1\n",
    "    temp = []\n",
    "\n",
    "    for i in frequent_set:\n",
    "        temp.append(i[0])\n",
    "\n",
    "    frequent_set = temp\n",
    "    for i in subset:\n",
    "        if i not in frequent_set:\n",
    "            flag=0\n",
    "            break\n",
    "\n",
    "    if flag:\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "def get_itemset(T):\n",
    "    result = set()\n",
    "    for i in range(len(T)):\n",
    "        if T[i]!=0:\n",
    "            result.add(i+1)\n",
    "\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42baff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DC = []\n",
    "DS = []\n",
    "SC = []\n",
    "SS = []\n",
    "\n",
    "for i in unique_itemset:\n",
    "    DC.append([i,0,0])\n",
    "\n",
    "print(\"Initial DC:\",DC,\"\\n\")\n",
    "\n",
    "counter = 0\n",
    "T = []\n",
    "while len(DC)!=0 or len(DS)!=0:\n",
    "\n",
    "    for i in range(counter,counter+M):\n",
    "        index = i%size\n",
    "        T = get_itemset(database[index])\n",
    "        print(\"Transaction :\",T)\n",
    "\n",
    "        for item in DC:\n",
    "            item[2]+=1\n",
    "            if item[0].issubset(T):\n",
    "                item[1]+=1\n",
    "        for item in DS:\n",
    "            item[2]+=1\n",
    "            if item[0].issubset(T):\n",
    "                item[1]+=1\n",
    "\n",
    "    for item in copy.copy(DC):\n",
    "        if(item[1]>=min_supp):\n",
    "            DS.append(item)\n",
    "            DC.remove(item)\n",
    "\n",
    "    for item in copy.copy(DS):\n",
    "        if(item[2]==size):\n",
    "            SS.append(item)\n",
    "            DS.remove(item)\n",
    "    for item in copy.copy(DC):\n",
    "        if(item[2]==size):\n",
    "            SC.append(item)\n",
    "            DC.remove(item)\n",
    "\n",
    "    frequent_set = copy.copy(DS)\n",
    "    frequent_set.extend(SS)\n",
    "    for item in frequent_set:\n",
    "        S = get_superset(item[0],unique_itemset)\n",
    "        for i in S:\n",
    "            if (check_subset(i,frequent_set)):\n",
    "                flag=1\n",
    "                for x in DC:\n",
    "                    if x[0]==i:\n",
    "                        flag=0\n",
    "                for x in DS:\n",
    "                    if x[0]==i:\n",
    "                        flag=0\n",
    "                for x in SC:\n",
    "                    if x[0]==i:\n",
    "                        flag=0\n",
    "                for x in SS:\n",
    "                    if x[0]==i:\n",
    "                        flag=0\n",
    "                if flag:\n",
    "                    DC.append([i,0,0])\n",
    "\n",
    "    counter+=M\n",
    "    print(\"DS: \",DS)\n",
    "    print(\"DC: \",DC)\n",
    "    print(\"SS: \",SS)\n",
    "    print(\"SC: \",SC,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08089a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
